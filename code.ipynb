{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated runtime:172.12s\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer as porter2\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from string import punctuation\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# loads text file data line by line into a list\n",
    "def txt_loader(file):\n",
    "    wrapper = open(file,\"r\",encoding=\"utf-8-sig\")\n",
    "    data = wrapper.readlines()\n",
    "    data = [line.strip() for line in data if line.strip()]\n",
    "    return data\n",
    "\n",
    "# preprocessor module: performs tokenisation, case-folding, stop-word removal and stemming.\n",
    "# set bool_flag=True so as not to remove stop-words 'and, 'or' and 'not' from a boolean query\n",
    "def preprocessor(raw,bool_flag=False):\n",
    "    data = re.split(r\"[^a-zA-Z0-9']+\",raw)\n",
    "    data = [re.sub(r\"^'|'$\",'',s) for s in data]\n",
    "    data = [tok.lower() for tok in data if len(tok)>0]\n",
    "    stop_words = txt_loader(\"englishST.txt\") \n",
    "    if bool_flag:\n",
    "        stop_words = set(stop_words) - set(['and','not','or'])\n",
    "    data = [tok for tok in data if tok not in stop_words]\n",
    "    stemmer = porter2(\"english\")\n",
    "    data = [stemmer.stem(tok) for tok in data]\n",
    "    return data\n",
    "\n",
    "# parses the XML tree returning a list of tuples representing our documents,\n",
    "# where tuple[0] is the document headline and text combined, and t[1] is the document's ID\n",
    "def xml_parse(root):\n",
    "    docIDs = []\n",
    "    headlines = []\n",
    "    text = []\n",
    "    for child in root.iter():\n",
    "        if child.tag == 'DOCNO':\n",
    "            docID = int(child.text)\n",
    "            docIDs.append(docID)\n",
    "        if child.tag == 'HEADLINE':\n",
    "            headlines.append(child.text)\n",
    "        if child.tag == 'TEXT':\n",
    "            text.append(child.text)\n",
    "    docs = [(h + t,ID) for h, t, ID in zip(headlines, text, docIDs)]\n",
    "    return docs\n",
    "\n",
    "# creates a positional inverted index from a list of parsed documents\n",
    "def create_index(docs):    \n",
    "    idx = {}\n",
    "    for c1, d in enumerate(docs):\n",
    "        tokens = preprocessor(d[0])\n",
    "        for c2, tok in enumerate(tokens):\n",
    "            if tok in idx:\n",
    "                prior = idx.get(tok)\n",
    "                idx[tok] = prior + [(d[1],c2)]\n",
    "            if tok not in idx:\n",
    "                idx[tok] = [(d[1],c2)]\n",
    "    return idx\n",
    "\n",
    "# returns all docIDs containing the input term, set negate_flag=True if returning all docIDs NOT containing term\n",
    "def bool_search(term,negate_flag=False):    \n",
    "    if term not in idx:\n",
    "        return []\n",
    "    ids = set([a[0] for a in idx[term]])\n",
    "    if negate_flag:\n",
    "        return sorted(set(docIDs)-ids)\n",
    "    else:\n",
    "        return sorted(ids)\n",
    "\n",
    "# returns docIDs containing the phrase given by 'term1 term2'\n",
    "def phrase_search(term1, term2):    \n",
    "    match_docs =[]\n",
    "    overlapIDs = intersection(bool_search(term1),bool_search(term2))\n",
    "    postings_1 = [i for i in idx[term1] if i[0] in overlapIDs]\n",
    "    postings_2 = [i for i in idx[term2] if i[0] in overlapIDs]\n",
    "    \n",
    "    for i in overlapIDs:\n",
    "        tpositions_1 = [a[1] for a in postings_1 if a[0]==i]\n",
    "        tpositions_2 = [a[1]-1 for a in postings_2 if a[0]==i]\n",
    "        if any(item in tpositions_1 for item in tpositions_2):\n",
    "            match_docs.append(i)\n",
    "            \n",
    "    return match_docs\n",
    "\n",
    "# returns docIDs for which term1 and term2 are at most d positions apart, after stop words have been removed\n",
    "def proximity_search(d,term1, term2):\n",
    "    d = int(d)\n",
    "    match_docs =[]\n",
    "    overlapIDs = intersection(bool_search(term1),bool_search(term2))\n",
    "    postings_1 = [i for i in idx[term1] if i[0] in overlapIDs]\n",
    "    postings_2 = [i for i in idx[term2] if i[0] in overlapIDs]\n",
    "    \n",
    "    for i in overlapIDs:\n",
    "        tpositions_1 = [a[1] for a in postings_1 if a[0]==i]\n",
    "        tpositions_2 = [a[1] for a in postings_2 if a[0]==i]\n",
    "        prox_check = [True if abs(x-y)<=d else False for x in tpositions_1 for y in tpositions_2]\n",
    "        if any(prox_check):\n",
    "            match_docs.append(i)\n",
    "            \n",
    "    return match_docs\n",
    "\n",
    "# Returns the common docIDs between two searches\n",
    "def intersection(list1,list2):\n",
    "    return sorted(list(set(list1).intersection(list2)))\n",
    "\n",
    "# Returns all docIDs retrieved by two searches\n",
    "def union(list1,list2):\n",
    "    return sorted(list(set(list1).union(list2)))\n",
    "\n",
    "# returns the list of docIDs for which the query occurs in\n",
    "def search(query):\n",
    "    # simple term search\n",
    "    if len(query)==1:\n",
    "        return bool_search(query[0])\n",
    "    # simple term search with negation\n",
    "    if len(query)==2 and query[0]=='not':\n",
    "        return bool_search(query[1],True)\n",
    "    # phrase search\n",
    "    if len(query)==2:\n",
    "        return phrase_search(query[0],query[1])\n",
    "    # first token is a number + absence of boolean operator but length of query>2 implies proximity search \n",
    "    if query[0].isnumeric() and 'or' not in query[1:] and 'and' not in query[1:]:\n",
    "        return proximity_search(query[0],query[1], query[2])\n",
    "    # otherwise search query contains boolean operators, which are identified by flags\n",
    "    and_flag = False\n",
    "    or_flag = False\n",
    "    if 'and' in query:\n",
    "        and_flag = True\n",
    "        split = query.index('and')\n",
    "    if 'or' in query:\n",
    "        or_flag = True\n",
    "        split = query.index('or')\n",
    "    # query is split where the boolean operator occurs, treated as separate queries then merged    \n",
    "    first = query[:split]\n",
    "    second = query[split+1:]  \n",
    "    res1=[]\n",
    "    res2=[]\n",
    "    if len(first)==1:\n",
    "        res1 = bool_search(first[0])\n",
    "    if len(second)==1:\n",
    "        res2 = bool_search(second[0])\n",
    "    if len(first)==2 and 'not' in first:\n",
    "        res1 = bool_search(first[1],True)\n",
    "    elif len(first)==2:\n",
    "        res1 = phrase_search(first[0],first[1])\n",
    "    if len(second)==2 and 'not' in second:\n",
    "        res2 = bool_search(second[1],True) \n",
    "    elif len(second)==2 and 'not' not in second:\n",
    "        res2 = phrase_search(second[0],second[1])\n",
    "    # merged by union or intersection depending on which operator was called\n",
    "    if and_flag:\n",
    "        return intersection(res1,res2)\n",
    "    if or_flag:\n",
    "        return union(res1,res2)\n",
    "\n",
    "# returns the count of appearances of a term in a document\n",
    "def tf(term,docID):\n",
    "    return len([a[0] for a in idx[term] if a[0]==docID])\n",
    "\n",
    "# returns the total number of documents containing the term\n",
    "def df(term):\n",
    "    return len(set([a[0] for a in idx[term]]))\n",
    "\n",
    "# returns the inverse domain frequency of a term\n",
    "def idf(term):\n",
    "    return np.log10(len(docIDs)/df(term))\n",
    "\n",
    "# returns the score based on TDIDF weight for a query phrase\n",
    "def score(query,docID):\n",
    "    return sum([(1+np.log10(tf(term,docID)))*idf(term) for term in query if tf(term,docID)>0])\n",
    "\n",
    "# sorts docID and score pairs by score to give highest scoring docID first for a given query\n",
    "def rank(query):\n",
    "    return sorted([(docID,score(query,docID)) for docID in docIDs if score(query,docID)>0],key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# writes the positional inverted index to \"index.txt\" in the specified format\n",
    "def index_out():\n",
    "    f = open(\"index.txt\",'w+')\n",
    "    for term in idx:\n",
    "        doc_freq = df(term)\n",
    "        f1 = sorted([(b,[a[1] for a in idx[term] if a[0]==b]) for b in set([a[0] for a in idx[term]])], key = lambda x : x[0])\n",
    "\n",
    "        f.write(\"{term}:{df}\\n\".format(term=term,df=doc_freq))\n",
    "        for i, posting_list in enumerate(f1):\n",
    "            f.write(\"\\t{docID}: \".format(docID=f1[i][0]))\n",
    "            for x, position in enumerate(f1[i][1]):\n",
    "                if x == len(f1[i][1])-1:\n",
    "                    f.write(\"{pos}\\n\".format(pos = position))\n",
    "                    break\n",
    "                f.write(\"{pos},\".format(pos = position))\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "# writes all positive resulting docIDs per query to \"results.boolean.txt\"\n",
    "def b_results(file):\n",
    "    f = open(\"results.boolean.txt\",\"w+\")\n",
    "    b_queries = txt_loader(file)\n",
    "    b_processed = [preprocessor(query,True) for query in b_queries]\n",
    "    b_processed = [q[1:] for q in b_processed]\n",
    "    for i,x in enumerate(b_processed,1):\n",
    "        for j in search(x):\n",
    "            f.write(\"{query},{docID}\\n\".format(query=i,docID=j))\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "# writes at most 150 docIDs and their score for a given query to \"results.ranked.txt\" with highest scoring first\n",
    "def r_results(file):\n",
    "    f = open(\"results.ranked.txt\",\"w+\")\n",
    "    r_queries = txt_loader(file)\n",
    "    r_processed = [preprocessor(q) for q in r_queries]\n",
    "    # remove query number from query\n",
    "    r_processed = [q[1:] for q in r_processed]\n",
    "    out=[]\n",
    "    for i,x in enumerate(r_processed,1):\n",
    "        for c,j in enumerate(rank(x)):\n",
    "            if c == 150:\n",
    "                break\n",
    "            ad = [i] + list(j)\n",
    "            out.append(ad)\n",
    "    out = [[a[0],a[1],round(a[2],4)] for a in out]\n",
    "    for en in out:\n",
    "        f.write(\"{query},{docID},{score}\\n\".format(query=en[0],docID=en[1],score=en[2]))\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "# create global variables\n",
    "\n",
    "root = ET.parse(os.getcwd() + '/trec.5000.xml').getroot()\n",
    "docs = xml_parse(root)\n",
    "idx = create_index(docs)\n",
    "\n",
    "docIDs = [d[1] for d in docs]\n",
    "terms = list(idx.keys())\n",
    "\n",
    "index_out()\n",
    "b_results(\"queries.boolean.txt\")\n",
    "r_results(\"queries.ranked.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
